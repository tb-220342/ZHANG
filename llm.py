# å¤§è¯­è¨€æ¨¡å‹æ¨¡å—
from vlm import *

try:
    from letta import create_client, LLMConfig, EmbeddingConfig
    from letta.schemas.memory import ChatMemory
except:
    pass
with open('data/db/memory.db', 'r', encoding='utf-8') as memory_file:
    try:
        openai_history = json.load(memory_file)
    except:
        openai_history = []
spark_history = []
sf_url = "https://api.siliconflow.cn/v1"


def chat_preprocess(msg):  # é¢„å¤„ç†
    try:
        content = "å›¾åƒè¯†åˆ«å·²å…³é—­"
        if ("å±å¹•" in msg or "ç”»é¢" in msg or "å›¾åƒ" in msg or "çœ‹åˆ°" in msg or "çœ‹è§" in msg or "ç…§ç‰‡" in msg or "æ‘„åƒå¤´" in msg or "å›¾ç‰‡" in msg) and img_menu.get() != "å…³é—­å›¾åƒè¯†åˆ«":
            if "å›¾ç‰‡" in msg:
                if os.path.exists("data/cache/cache.png"):
                    if img_menu.get() == "GLM-4V-Flash":
                        content = glm_4v_photo(msg)
                    elif img_menu.get() == "æœ¬åœ°Ollama VLM":
                        content = ollama_vlm_photo(msg)
                    elif img_menu.get() == "æœ¬åœ°QwenVLæ•´åˆåŒ…":
                        content = qwen_vlm_photo(msg)
                    elif img_menu.get() == "æœ¬åœ°GLM-Væ•´åˆåŒ…":
                        content = glm_v_photo(msg)
                    elif img_menu.get() == "æœ¬åœ°Janusæ•´åˆåŒ…":
                        content = janus_photo(msg)
                    elif img_menu.get() == "è‡ªå®šä¹‰API-VLM":
                        content = custom_vlm_photo(msg)
                    notice(f"{mate_name}è¯†åˆ«äº†ä¸Šä¼ çš„å›¾ç‰‡")
                    os.remove("data/cache/cache.png")
                else:
                    content = "è¯·å…ˆç‚¹å‡»å³ä¸‹æ–¹æŒ‰é’®ä¸Šä¼ å›¾ç‰‡"
                    notice("è¯·å…ˆç‚¹å‡»å³ä¸‹æ–¹æŒ‰é’®ä¸Šä¼ å›¾ç‰‡")
            elif "å±å¹•" in msg or "ç”»é¢" in msg or "å›¾åƒ" in msg:
                if img_menu.get() == "GLM-4V-Flash":
                    content = glm_4v_screen(msg)
                elif img_menu.get() == "æœ¬åœ°Ollama VLM":
                    content = ollama_vlm_screen(msg)
                elif img_menu.get() == "æœ¬åœ°QwenVLæ•´åˆåŒ…":
                    content = qwen_vlm_screen(msg)
                elif img_menu.get() == "æœ¬åœ°GLM-Væ•´åˆåŒ…":
                    content = glm_v_screen(msg)
                elif img_menu.get() == "æœ¬åœ°Janusæ•´åˆåŒ…":
                    content = janus_screen(msg)
                elif img_menu.get() == "è‡ªå®šä¹‰API-VLM":
                    content = custom_vlm_screen(msg)
                notice(f"{mate_name}æ•è·äº†å±å¹•ï¼Œè°ƒç”¨[ç”µè„‘å±å¹•è¯†åˆ«]")
            elif "çœ‹åˆ°" in msg or "çœ‹è§" in msg or "ç…§ç‰‡" in msg or "æ‘„åƒå¤´" in msg:
                if img_menu.get() == "GLM-4V-Flash":
                    content = glm_4v_cam(msg)
                elif img_menu.get() == "æœ¬åœ°Ollama VLM":
                    content = ollama_vlm_cam(msg)
                elif img_menu.get() == "æœ¬åœ°QwenVLæ•´åˆåŒ…":
                    content = qwen_vlm_cam(msg)
                elif img_menu.get() == "æœ¬åœ°GLM-Væ•´åˆåŒ…":
                    content = glm_v_cam(msg)
                elif img_menu.get() == "æœ¬åœ°Janusæ•´åˆåŒ…":
                    content = janus_cam(msg)
                elif img_menu.get() == "è‡ªå®šä¹‰API-VLM":
                    content = custom_vlm_cam(msg)
                notice(f"{mate_name}æ‹äº†ç…§ç‰‡ï¼Œè°ƒç”¨[æ‘„åƒå¤´è¯†åˆ«]")
        else:
            content = chat_llm(prompt, msg)
            notice(f"æ”¶åˆ°{mate_name}å›å¤")
        with open('data/db/memory.db', 'w', encoding='utf-8') as memory_file:
            json.dump(openai_history, memory_file, ensure_ascii=False, indent=4)
        return content
    except Exception as e:
        notice(f"å‘ç”Ÿé”™è¯¯ï¼Œé”™è¯¯è¯¦æƒ…ï¼š{e}")
        return f"å‘ç”Ÿé”™è¯¯ï¼Œé”™è¯¯è¯¦æƒ…ï¼š{e}"


def chat_llm(tishici, msg):  # å¤§è¯­è¨€æ¨¡å‹èŠå¤©
    try:
        if llm_menu.get() == "è®¯é£æ˜Ÿç«Lite":
            spark_client = OpenAI(base_url="https://spark-api-open.xf-yun.com/v1", api_key=spark_key)
            spark_history.append({"role": "user", "content": f"{tishici}ã€‚æˆ‘çš„é—®é¢˜æ˜¯ï¼š{msg}"})
            messages = []
            messages.extend(spark_history)
            completion = spark_client.chat.completions.create(model="general", messages=messages)
            spark_history.append({"role": "assistant", "content": completion.choices[0].message.content})
            return completion.choices[0].message.content
        elif llm_menu.get() == "GLM-4-Flash":
            glm_client = OpenAI(base_url=glm_url, api_key=glm_key)
            openai_history.append({"role": "user", "content": msg})
            messages = [{"role": "system", "content": tishici}]
            messages.extend(openai_history)
            completion = glm_client.chat.completions.create(model="glm-4-flash-250414", messages=messages)
            openai_history.append({"role": "assistant", "content": completion.choices[0].message.content})
            return completion.choices[0].message.content
        elif llm_menu.get() == "GLM-Z1-Flash":
            glm_client = OpenAI(base_url=glm_url, api_key=glm_key)
            openai_history.append({"role": "user", "content": msg})
            messages = [{"role": "system", "content": tishici}]
            messages.extend(openai_history)
            completion = glm_client.chat.completions.create(model="glm-z1-flash", messages=messages)
            openai_history.append({"role": "assistant", "content": completion.choices[0].message.content})
            return completion.choices[0].message.content
        elif llm_menu.get() == "DeepSeek-R1-7B":
            ds_client = OpenAI(base_url=sf_url, api_key=sf_key)
            openai_history.append({"role": "user", "content": msg})
            messages = [{"role": "system", "content": tishici}]
            messages.extend(openai_history)
            completion = ds_client.chat.completions.create(model="deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
                                                           messages=messages)
            openai_history.append({"role": "assistant", "content": completion.choices[0].message.content})
            return completion.choices[0].message.content
        elif llm_menu.get() == "é€šä¹‰åƒé—®2.5-7B":
            qwen_client = OpenAI(base_url=sf_url, api_key=sf_key)
            openai_history.append({"role": "user", "content": msg})
            messages = [{"role": "system", "content": tishici}]
            messages.extend(openai_history)
            completion = qwen_client.chat.completions.create(model="Qwen/Qwen2.5-7B-Instruct",
                                                             messages=messages)
            openai_history.append({"role": "assistant", "content": completion.choices[0].message.content})
            return completion.choices[0].message.content
        elif llm_menu.get() == "InternLM2.5-7B":
            internlm_client = OpenAI(base_url=sf_url, api_key=sf_key)
            openai_history.append({"role": "user", "content": msg})
            messages = [{"role": "system", "content": tishici}]
            messages.extend(openai_history)
            completion = internlm_client.chat.completions.create(model="internlm/internlm2_5-7b-chat",
                                                                 messages=messages)
            openai_history.append({"role": "assistant", "content": completion.choices[0].message.content})
            return completion.choices[0].message.content
        elif llm_menu.get() == "è…¾è®¯æ··å…ƒLite":
            hunyuan_client = OpenAI(base_url="https://api.hunyuan.cloud.tencent.com/v1", api_key=hy_key)
            openai_history.append({"role": "user", "content": msg})
            messages = [{"role": "system", "content": tishici}]
            messages.extend(openai_history)
            completion = hunyuan_client.chat.completions.create(model="hunyuan-lite", messages=messages)
            openai_history.append({"role": "assistant", "content": completion.choices[0].message.content})
            return completion.choices[0].message.content
        elif llm_menu.get() == "æœ¬åœ°Qwenæ•´åˆåŒ…":
            api = f"http://{local_server_ip}:8088/llm/?p={tishici}&q={msg}"
            try:
                res = rq.get(api).json()["answer"]
                return res
            except Exception as e:
                return f"æœ¬åœ°Qwenæ•´åˆåŒ…APIæœåŠ¡å™¨æœªå¼€å¯ï¼Œé”™è¯¯è¯¦æƒ…ï¼š{str(e)[0:100]}"
        elif llm_menu.get() == "æœ¬åœ°LM Studio":
            try:
                lmstudio_client = OpenAI(base_url=f"http://{local_server_ip}:{lmstudio_port}/v1", api_key="lm-studio")
                openai_history.append({"role": "user", "content": msg})
                messages = [{"role": "system", "content": tishici}]
                messages.extend(openai_history)
                completion = lmstudio_client.chat.completions.create(model="", messages=messages)
                openai_history.append({"role": "assistant", "content": completion.choices[0].message.content})
                return completion.choices[0].message.content
            except Exception as e:
                return f"æœ¬åœ°LM Studioè½¯ä»¶APIæœåŠ¡æœªå¼€å¯ï¼Œé”™è¯¯è¯¦æƒ…ï¼š{e}"
        elif llm_menu.get() == "æœ¬åœ°Ollama LLM":
            try:
                try:
                    rq.get(f'http://{local_server_ip}:{ollama_port}')
                except:
                    Popen(f"ollama pull {ollama_model_name}", shell=False)
                ollama_client = Client(host=f'http://{local_server_ip}:{ollama_port}')
                openai_history.append({"role": "user", "content": msg})
                messages = [{"role": "system", "content": tishici}]
                messages.extend(openai_history)
                response = ollama_client.chat(model=ollama_model_name, messages=messages)
                openai_history.append({"role": "assistant", "content": response['message']['content']})
                return response['message']['content']
            except Exception as e:
                return f"æœ¬åœ°Ollama LLMé…ç½®é”™è¯¯ï¼Œé”™è¯¯è¯¦æƒ…ï¼š{e}"
        elif llm_menu.get() == "æœ¬åœ°RWKVè¿è¡Œå™¨":
            try:
                rwkv_client = OpenAI(base_url=f"http://{local_server_ip}:8000/v1", api_key="rwkv")
                openai_history.append({"role": "user", "content": msg})
                messages = [{"role": "system", "content": tishici}]
                messages.extend(openai_history)
                completion = rwkv_client.chat.completions.create(model="rwkv", messages=messages)
                openai_history.append({"role": "assistant", "content": completion.choices[0].message.content})
                return completion.choices[0].message.content
            except Exception as e:
                return f"æœ¬åœ°RWKV Runnerè½¯ä»¶APIæœåŠ¡æœªå¼€å¯ï¼Œé”™è¯¯è¯¦æƒ…ï¼š{e}"
        elif llm_menu.get() == "æœ¬åœ°OpenVINO":
            api = f"http://{local_server_ip}:8087/openvino/?p={tishici}&q={msg}"
            try:
                res = rq.get(api).json()["answer"]
                return res
            except Exception as e:
                return f"æœ¬åœ°OpenVINOæ•´åˆåŒ…APIæœåŠ¡å™¨æœªå¼€å¯ï¼Œé”™è¯¯è¯¦æƒ…ï¼š{str(e)[0:100]}"
        elif llm_menu.get() == "DifyèŠå¤©åŠ©æ‰‹":
            try:
                res = chat_dify(msg)
                return res
            except Exception as e:
                return f"æœ¬åœ°DifyèŠå¤©åŠ©æ‰‹é…ç½®é”™è¯¯ï¼Œé”™è¯¯è¯¦æƒ…ï¼š{e}"
        elif llm_menu.get() == "AnythingLLM":
            try:
                res = chat_anything_llm(msg)
                return res
            except Exception as e:
                return f"æœ¬åœ°AnythingLLMçŸ¥è¯†åº“é…ç½®é”™è¯¯ï¼Œé”™è¯¯è¯¦æƒ…ï¼š{e}"
        elif llm_menu.get() == "Lettaé•¿æœŸè®°å¿†":
            res = chat_letta(msg)
            return res
        else:
            try:
                custom_client = OpenAI(base_url=custom_url, api_key=custom_key)
                openai_history.append({"role": "user", "content": msg})
                messages = [{"role": "system", "content": tishici}]
                messages.extend(openai_history)
                completion = custom_client.chat.completions.create(model=custom_model, messages=messages)
                openai_history.append({"role": "assistant", "content": completion.choices[0].message.content})
                return completion.choices[0].message.content
            except Exception as e:
                return f"è‡ªå®šä¹‰APIé…ç½®é”™è¯¯ï¼Œé”™è¯¯è¯¦æƒ…ï¼š{e}"
    except Exception as e:
        notice(f"{llm_menu.get()}ä¸å¯ç”¨ï¼Œè¯·å‰å¾€è½¯ä»¶è®¾ç½®æ­£ç¡®é…ç½®äº‘ç«¯AI Keyï¼Œé”™è¯¯è¯¦æƒ…ï¼š{e}")
        return f"{llm_menu.get()}ä¸å¯ç”¨ï¼Œè¯·å‰å¾€è½¯ä»¶è®¾ç½®æ­£ç¡®é…ç½®äº‘ç«¯AI Keyï¼Œé”™è¯¯è¯¦æƒ…ï¼š{e}"


def chat_dify(msg):  # DifyèŠå¤©åŠ©æ‰‹
    headers = {"Authorization": f"Bearer {dify_key}", "Content-Type": "application/json"}
    data = {"query": msg, "inputs": {}, "response_mode": "blocking", "user": username, "conversation_id": None}
    res = rq.post(f"http://{dify_ip}/v1/chat-messages", headers=headers, data=json.dumps(data))
    res = res.json()['answer'].strip()
    return res


def chat_anything_llm(msg):  # AnythingLLMçŸ¥è¯†åº“
    url = f"http://{local_server_ip}:3001/api/v1/workspace/{anything_llm_ws}/chat"
    headers = {"Authorization": f"Bearer {anything_llm_key}", "Content-Type": "application/json"}
    data = {"message": msg}
    res = rq.post(url, json=data, headers=headers)
    return res.json().get("textResponse")


def chat_letta(msg):  # Lettaé•¿æœŸè®°å¿†
    answer = "Lettaé•¿æœŸè®°å¿†æœåŠ¡æ‹¥æŒ¤ï¼Œè¯·ä¸€æ®µæ—¶é—´åå†è¯•"
    try:
        client = create_client()
        with open('data/db/letta.db', 'r', encoding='utf-8') as f:
            agent_state_id = f.read()
        if len(agent_state_id) < 10:
            agent_state = client.create_agent(
                memory=ChatMemory(persona=prompt, human=f"Name: {username}"),
                llm_config=LLMConfig.default_config(model_name="letta"),
                embedding_config=EmbeddingConfig.default_config(model_name="text-embedding-ada-002"))
            with open('data/db/letta.db', 'w', encoding='utf-8') as f:
                f.write(agent_state.id)
            agent_state_id = agent_state.id
        response = client.send_message(
            agent_id=agent_state_id, role="user", message=f"[{current_time()}]{msg}")
        result = response.messages
        for message in result:
            if message.message_type == 'tool_call_message':
                function_arguments = message.tool_call.arguments
                if function_arguments:
                    arguments_dict = json.loads(function_arguments)
                    answer = arguments_dict.get("message")
                    break
    except:
        answer = "Lettaé•¿æœŸè®°å¿†å‡ºç°å…¼å®¹æ€§é—®é¢˜æš‚ä¸å¯ç”¨ï¼Œå¯æ›´æ¢å…¶ä»–å¯¹è¯æ¨¡å‹"
    return answer


def clear_chat():  # æ¸…é™¤å¯¹è¯è®°å½•
    global openai_history, spark_history
    if messagebox.askokcancel(f"æ¸…é™¤{mate_name}çš„è®°å¿†å’ŒèŠå¤©è®°å½•",
                              f"æ‚¨ç¡®å®šè¦æ¸…é™¤{mate_name}çš„è®°å¿†å’ŒèŠå¤©è®°å½•å—ï¼Ÿ\nå¦‚æœ‰éœ€è¦å¯å…ˆç‚¹å‡»ğŸ”¼å¯¼å‡ºè®°å½•å†å¼€å¯æ–°å¯¹è¯"):
        output_box.delete("1.0", "end")
        openai_history, spark_history = [], []
        with open('data/db/letta.db', 'w', encoding="utf-8") as f:
            f.write("0")
        with open('data/db/memory.db', 'w', encoding='utf-8') as f:
            f.write("")
        notice("è®°å¿†å’ŒèŠå¤©è®°å½•å·²æ¸…ç©º")


def clean_chat_web():  # æ¸…é™¤å¯¹è¯è®°å½•
    global openai_history, spark_history
    openai_history, spark_history = [], []
