{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tb-220342/ZHANG/blob/master/nb/Gemma3_(4B).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "he-8sk6zkbxS"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + â­ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> â­\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XcycHwhzwgW"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky4QqIC3kbxT"
      },
      "source": [
        "**Read our [Gemma 3 blog](https://unsloth.ai/blog/gemma3) for what's new in Unsloth and our [Reasoning blog](https://unsloth.ai/blog/r1-reasoning) on how to train reasoning models.**\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"CoT.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1pSPKk3QmWFqT5VrixE1GOb_4SpkG_2YR\n",
        "\"\"\"\n",
        "\n",
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "# %%capture\n",
        "!pip install unsloth\n",
        "!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
        "#\n",
        "\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "hf_token = userdata.get('HF')\n",
        "\n",
        "login(hf_token)\n",
        "\n",
        "# å¯¼å…¥wandbåº“ - Weights & Biasesï¼Œç”¨äºæœºå™¨å­¦ä¹ å®éªŒè·Ÿè¸ªå’Œå¯è§†åŒ–\n",
        "import wandb\n",
        "\n",
        "wb_token = userdata.get('WB')\n",
        "\n",
        "wandb.login(key=wb_token)\n",
        "run = wandb.init(\n",
        "    project='gemma',\n",
        "    job_type=\"training\",\n",
        "    anonymous=\"allow\"# å…è®¸åŒ¿åè®¿é—® # \"allow\"è¡¨ç¤ºå³ä½¿æ²¡æœ‰wandbè´¦å·çš„ç”¨æˆ·ä¹Ÿèƒ½æŸ¥çœ‹è¿™ä¸ªé¡¹ç›®\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "Adf7pZAFniPp",
        "outputId": "796553f9-4e8e-417d-f29c-e28910d535c5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unsloth in /usr/local/lib/python3.11/dist-packages (2025.3.14)\n",
            "Collecting git+https://github.com/unslothai/unsloth.git\n",
            "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-req-build-fbm3pt1f\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-req-build-fbm3pt1f\n",
            "  Resolved https://github.com/unslothai/unsloth.git to commit 6f7c8c6d0a63caaa129cc0bc6b845d5d8b9c81e8\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: unsloth\n",
            "  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unsloth: filename=unsloth-2025.3.14-py3-none-any.whl size=194396 sha256=ec26fd448d2ed2f1fc90c6cb4f63d3434e26af6c8986ec4bd7c6e941cbd03f20\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-re56lf8l/wheels/d1/17/05/850ab10c33284a4763b0595cd8ea9d01fce6e221cac24b3c01\n",
            "Successfully built unsloth\n",
            "Installing collected packages: unsloth\n",
            "  Attempting uninstall: unsloth\n",
            "    Found existing installation: unsloth 2025.3.14\n",
            "    Uninstalling unsloth-2025.3.14:\n",
            "      Successfully uninstalled unsloth-2025.3.14\n",
            "Successfully installed unsloth-2025.3.14\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "unsloth"
                ]
              },
              "id": "945510ac23dc4240b0e26c703260c2cd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ä»unslothåº“ä¸­å¯¼å…¥FastLanguageModelç±»\n",
        "# unslothæ˜¯ä¸€ä¸ªä¼˜åŒ–çš„è¯­è¨€æ¨¡å‹åŠ è½½å’Œè®­ç»ƒåº“\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# è®¾ç½®æ¨¡å‹å‚æ•°\n",
        "# æœ€å¤§åºåˆ—é•¿åº¦ï¼Œå³æ¨¡å‹èƒ½å¤„ç†çš„æœ€å¤§tokenæ•°é‡\n",
        "max_seq_length = 2048\n",
        "dtype = None # æ•°æ®ç±»å‹è®¾ç½®ä¸ºNoneï¼Œè®©æ¨¡å‹è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„æ•°æ®ç±»å‹\n",
        "load_in_4bit = True # å¯ç”¨4bité‡åŒ–åŠ è½½ # 4bité‡åŒ–å¯ä»¥æ˜¾è‘—å‡å°‘æ¨¡å‹å†…å­˜å ç”¨ï¼Œä½†å¯èƒ½ç•¥å¾®å½±å“æ¨¡å‹æ€§èƒ½\n",
        "\n",
        "# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/gemma-3-4b-it\",\n",
        "    max_seq_length = max_seq_length,     # è®¾ç½®æœ€å¤§åºåˆ—é•¿åº¦\n",
        "    dtype = dtype,# è®¾ç½®æ•°æ®ç±»å‹\n",
        "    load_in_4bit = load_in_4bit,  # å¯ç”¨4bité‡åŒ–åŠ è½½\n",
        "    token = hf_token, # ä½¿ç”¨Hugging Faceçš„è®¿é—®ä»¤ç‰Œ\n",
        ")\n",
        "\n",
        "prompt = \"\"\"Below is SQL query. Think like sql expert and generate a summary of the query which explains the use case of the query. As in\n",
        "what the query is trying to read from the database in a usecase sense.\n",
        "\n",
        "### Query:\n",
        "SELECT\n",
        "    c.customer_id,\n",
        "    c.name AS customer_name,\n",
        "    COUNT(o.order_id) AS total_orders,\n",
        "    SUM(o.total_amount) AS total_spent,\n",
        "    AVG(o.total_amount) AS avg_order_value,\n",
        "    MAX(o.order_date) AS last_order_date\n",
        "FROM customers c\n",
        "JOIN orders o ON c.customer_id = o.customer_id\n",
        "LEFT JOIN order_items oi ON o.order_id = oi.order_id\n",
        "WHERE o.order_date >= '2024-01-01'\n",
        "GROUP BY c.customer_id, c.name\n",
        "ORDER BY total_spent DESC\n",
        "LIMIT 10;\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "# å°†æ¨¡å‹åˆ‡æ¢åˆ°æ¨ç†æ¨¡å¼\n",
        "FastLanguageModel.for_inference(model)\n",
        "# å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œåˆ†è¯å’Œç¼–ç \n",
        "# [prompt] - å°†promptæ”¾å…¥åˆ—è¡¨ä¸­ï¼Œå› ä¸ºtokenizeræœŸæœ›æ‰¹å¤„ç†è¾“å…¥\n",
        "# return_tensors=\"pt\" - è¿”å›PyTorchå¼ é‡æ ¼å¼\n",
        "# to(\"cuda\") - å°†å¼ é‡ç§»åŠ¨åˆ°GPUä¸Šè¿›è¡Œè®¡ç®—\n",
        "inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# ä½¿ç”¨æ¨¡å‹ç”Ÿæˆå“åº”\n",
        "outputs = model.generate(\n",
        "    input_ids=inputs.input_ids,\n",
        "    attention_mask=inputs.attention_mask,\n",
        "    max_new_tokens=1200,\n",
        "    use_cache=True,\n",
        ")\n",
        "# åå¤„ç†ç”Ÿæˆçš„è¾“å‡º\n",
        "# batch_decode - å°†æ ‡è®°IDåºåˆ—è§£ç å›æ–‡æœ¬\n",
        "# split(\"### Response:\")[1] - æå–\"### Response:\"ä¹‹åçš„éƒ¨åˆ†ï¼Œå³æ¨¡å‹çš„å®é™…å›ç­”\n",
        "response = tokenizer.batch_decode(outputs)\n",
        "print(response[0].split(\"### Response:\")[1])\n",
        "\n",
        "#å¾®è°ƒå‰ï¼šæ¨¡å‹èƒ½ç»™å‡ºåŸºæœ¬çš„æŸ¥è¯¢è§£é‡Šï¼Œä½†ä¸å¤Ÿç²¾ç¡®\n",
        "\n",
        "train_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n",
        "Write a response that appropriately completes the request.\n",
        "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
        "\n",
        "### Instruction:\n",
        "You are a SQL expert with advance understanding of SQL queries. You can understand database schema from the query. Think like sql expert and generate a summary of the query which explains the use case of the query. As in\n",
        "what the query is trying to read from the database in a usecase sense.\n",
        "\n",
        "### Query:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "<think>\n",
        "{}\n",
        "</think>\n",
        "{}\"\"\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "-OD_R_rRlJc5",
        "outputId": "948d50bf-6b23-4a67-891b-759b84e0933a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'hf_token' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1ac85a77a8d6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m# è®¾ç½®æ•°æ®ç±»å‹\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mload_in_4bit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_in_4bit\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# å¯ç”¨4bité‡åŒ–åŠ è½½\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhf_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# ä½¿ç”¨Hugging Faceçš„è®¿é—®ä»¤ç‰Œ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'hf_token' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!wget \"https://huggingface.co/datasets/b-mc2/sql-create-context/resolve/main/sql_create_context_v4.json\"\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"json\", data_files=\"/content/sql_create_context_v4.json\", split=\"train[0:1000]\")\n",
        "# - split: æŒ‡å®šè¦åŠ è½½çš„æ•°æ®é›†åˆ‡ç‰‡\n",
        "#   - \"train[0:500]\" è¡¨ç¤ºåªåŠ è½½è®­ç»ƒé›†çš„å‰500æ¡æ•°æ®\n",
        "#   - è¿™ç§åˆ‡ç‰‡æ–¹å¼å¯ä»¥ç”¨äºå¿«é€Ÿå®éªŒå’Œè°ƒè¯•\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n",
        "\n",
        "#åŸæ•°æ®é›†ï¼šè¾“å…¥æ˜¯è‹±æ–‡æè¿°ï¼Œè¾“å‡ºæ˜¯SQLåè½¬åï¼šè¾“å…¥æ˜¯SQL(examples[\"answer\"])ï¼Œè¾“å‡ºæ˜¯è‹±æ–‡æè¿°(examples[\"question\"])\n",
        "def switch_and_format_prompt(examples):\n",
        "    inputs = examples[\"question\"] # ä½¿ç”¨ answer(SQL) ä½œä¸ºè¾“å…¥\n",
        "    context = examples[\"context\"]\n",
        "    outputs = examples[\"answer\"] # ä½¿ç”¨ question(è‹±æ–‡æè¿°) ä½œä¸ºè¾“å‡º\n",
        "    texts = []\n",
        "    for input, context, output in zip(inputs, context, outputs):\n",
        "        text = train_prompt_style.format(input, context, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return {\n",
        "        \"text\": texts,\n",
        "    }\n",
        "\n",
        "# åº”ç”¨è½¬æ¢\n",
        "dataset = dataset.map(switch_and_format_prompt, batched = True)\n",
        "\n"
      ],
      "metadata": {
        "id": "9I3Ge_7knrIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,   # LoRAçš„ç§©(rank)å€¼ï¼Œå†³å®šäº†ä½ç§©çŸ©é˜µçš„ç»´åº¦ï¼Œè¾ƒå¤§çš„rå€¼(å¦‚16)å¯ä»¥æä¾›æ›´å¼ºçš„æ¨¡å‹è¡¨è¾¾èƒ½åŠ›ï¼Œä½†ä¼šå¢åŠ å‚æ•°é‡å’Œè®¡ç®—å¼€é”€ï¼Œè¾ƒå°çš„rå€¼(å¦‚4æˆ–8)åˆ™ä¼šå‡å°‘å‚æ•°é‡ï¼Œä½†å¯èƒ½å½±å“æ¨¡å‹æ€§èƒ½ï¼Œé€šå¸¸åœ¨4-16ä¹‹é—´é€‰æ‹©,éœ€è¦åœ¨æ€§èƒ½å’Œæ•ˆç‡ä¹‹é—´æƒè¡¡\n",
        "    target_modules=[ #æŒ‡å®šéœ€è¦åº”ç”¨LoRAå¾®è°ƒçš„æ¨¡å—åˆ—è¡¨ï¼Œq_proj, k_proj, v_proj: æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„æŸ¥è¯¢ã€é”®ã€å€¼æŠ•å½±å±‚\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\", #æ³¨æ„åŠ›è¾“å‡ºæŠ•å½±å±‚\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha=16, #ç¼©æ”¾å‚æ•°ï¼Œç”¨äºæ§åˆ¶LoRAæ›´æ–°çš„å¼ºåº¦ï¼Œé€šå¸¸è®¾ç½®ä¸ºä¸rç›¸åŒçš„å€¼ï¼Œè¾ƒå¤§çš„alphaä¼šå¢åŠ LoRAçš„å½±å“åŠ›ï¼Œè¾ƒå°çš„alphaåˆ™ä¼šå‡å¼±LoRAçš„å½±å“\n",
        "    lora_dropout=0, #LoRAå±‚çš„dropoutç‡ï¼Œ0è¡¨ç¤ºä¸ä½¿ç”¨dropoutï¼Œå¢åŠ dropoutå¯ä»¥å¸®åŠ©é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œä½†å¯èƒ½å½±å“è®­ç»ƒç¨³å®šæ€§ï¼Œåœ¨å¾®è°ƒæ—¶é€šå¸¸è®¾ä¸º0æˆ–å¾ˆå°çš„å€¼\n",
        "    bias=\"none\", #æ˜¯å¦å¾®è°ƒåç½®é¡¹ï¼Œ\"none\"è¡¨ç¤ºä¸å¾®è°ƒåç½®å‚æ•°ï¼Œä¹Ÿå¯ä»¥è®¾ç½®ä¸º\"all\"æˆ–\"lora_only\"æ¥å¾®è°ƒä¸åŒèŒƒå›´çš„åç½®\n",
        "    use_gradient_checkpointing=\"unsloth\",  # æ¢¯åº¦æ£€æŸ¥ç‚¹ç­–ç•¥ï¼Œ\"unsloth\"æ˜¯ä¸€ç§ä¼˜åŒ–çš„æ£€æŸ¥ç‚¹ç­–ç•¥ï¼Œé€‚ç”¨äºé•¿ä¸Šä¸‹æ–‡å¯ä»¥æ˜¾è‘—å‡å°‘æ˜¾å­˜ä½¿ç”¨ï¼Œä½†ä¼šç•¥å¾®å¢åŠ è®¡ç®—æ—¶é—´å¯¹å¤„ç†é•¿æ–‡æœ¬ç‰¹åˆ«æœ‰ç”¨\n",
        "    random_state=3407, #éšæœºæ•°ç§å­ï¼Œæ§åˆ¶åˆå§‹åŒ–çš„éšæœºæ€§ï¼Œå›ºå®šç§å­å¯ä»¥ç¡®ä¿å®éªŒå¯é‡å¤æ€§\n",
        "    use_rslora=False, #æ˜¯å¦ä½¿ç”¨RSLoRA(Rank-Stabilized LoRA) Falseè¡¨ç¤ºä½¿ç”¨æ ‡å‡†LoRARSLoRAæ˜¯ä¸€ç§æ”¹è¿›çš„LoRAå˜ä½“ï¼Œå¯ä»¥æä¾›æ›´ç¨³å®šçš„è®­ç»ƒ\n",
        "    loftq_config=None, #LoftQé…ç½®Noneè¡¨ç¤ºä¸ä½¿ç”¨LoftQé‡åŒ–LoftQæ˜¯ä¸€ç§ç”¨äºæ¨¡å‹é‡åŒ–çš„æŠ€æœ¯ï¼Œå¯ä»¥å‡å°‘æ¨¡å‹å¤§å°\n",
        ")\n",
        "\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    ## è®­ç»ƒå‚æ•°é…ç½®\n",
        "    args=TrainingArguments(\n",
        "        # æ‰¹å¤„ç†ç›¸å…³\n",
        "        per_device_train_batch_size=2, # æ¯ä¸ªè®¾å¤‡ï¼ˆGPUï¼‰çš„è®­ç»ƒæ‰¹æ¬¡å¤§å°\n",
        "        gradient_accumulation_steps=4,# æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼Œç”¨äºæ¨¡æ‹Ÿæ›´å¤§çš„æ‰¹æ¬¡å¤§å°\n",
        "         # è®­ç»ƒæ­¥æ•°å’Œé¢„çƒ­\n",
        "        warmup_steps=10,# å­¦ä¹ ç‡é¢„çƒ­æ­¥æ•°ï¼Œé€æ­¥å¢åŠ å­¦ä¹ ç‡\n",
        "        max_steps=60,# æœ€å¤§è®­ç»ƒæ­¥æ•°\n",
        "        learning_rate=5e-5,\n",
        "        fp16=not is_bfloat16_supported(), # å¦‚æœä¸æ”¯æŒ bfloat16ï¼Œåˆ™ä½¿ç”¨ float16\n",
        "        bf16=is_bfloat16_supported(),# å¦‚æœæ”¯æŒåˆ™ä½¿ç”¨ bfloat16ï¼Œé€šå¸¸åœ¨æ–°å‹ GPU ä¸Šæ€§èƒ½æ›´å¥½\n",
        "        logging_steps=10,# æ¯10æ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—\n",
        "        optim=\"adamw_8bit\", # ä½¿ç”¨8ä½ç²¾åº¦çš„ AdamW ä¼˜åŒ–å™¨\n",
        "        weight_decay=0.1,# æƒé‡è¡°å‡ç‡ï¼Œç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆ\n",
        "        lr_scheduler_type=\"linear\",# å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹ï¼Œä½¿ç”¨çº¿æ€§è¡°å‡\n",
        "        seed=3407,# éšæœºç§å­ï¼Œç¡®ä¿å®éªŒå¯é‡å¤æ€§\n",
        "        output_dir=\"outputs\", # æ¨¡å‹å’Œæ£€æŸ¥ç‚¹çš„è¾“å‡ºç›®å½•\n",
        "    ),\n",
        ")\n",
        "\n",
        "trainer_stats = trainer.train()\n"
      ],
      "metadata": {
        "id": "_3jw6xRenuir"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}