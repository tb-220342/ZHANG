{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tb-220342/ZHANG/blob/master/nb/Gemma3_(4B).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "he-8sk6zkbxS"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XcycHwhzwgW"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky4QqIC3kbxT"
      },
      "source": [
        "**Read our [Gemma 3 blog](https://unsloth.ai/blog/gemma3) for what's new in Unsloth and our [Reasoning blog](https://unsloth.ai/blog/r1-reasoning) on how to train reasoning models.**\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"CoT.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1pSPKk3QmWFqT5VrixE1GOb_4SpkG_2YR\n",
        "\"\"\"\n",
        "\n",
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "# %%capture\n",
        "!pip install unsloth\n",
        "!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
        "#\n",
        "\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "hf_token = userdata.get('HF')\n",
        "\n",
        "login(hf_token)\n",
        "\n",
        "# 导入wandb库 - Weights & Biases，用于机器学习实验跟踪和可视化\n",
        "import wandb\n",
        "\n",
        "wb_token = userdata.get('WB')\n",
        "\n",
        "wandb.login(key=wb_token)\n",
        "run = wandb.init(\n",
        "    project='gemma',\n",
        "    job_type=\"training\",\n",
        "    anonymous=\"allow\"# 允许匿名访问 # \"allow\"表示即使没有wandb账号的用户也能查看这个项目\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "Adf7pZAFniPp",
        "outputId": "796553f9-4e8e-417d-f29c-e28910d535c5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unsloth in /usr/local/lib/python3.11/dist-packages (2025.3.14)\n",
            "Collecting git+https://github.com/unslothai/unsloth.git\n",
            "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-req-build-fbm3pt1f\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-req-build-fbm3pt1f\n",
            "  Resolved https://github.com/unslothai/unsloth.git to commit 6f7c8c6d0a63caaa129cc0bc6b845d5d8b9c81e8\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: unsloth\n",
            "  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unsloth: filename=unsloth-2025.3.14-py3-none-any.whl size=194396 sha256=ec26fd448d2ed2f1fc90c6cb4f63d3434e26af6c8986ec4bd7c6e941cbd03f20\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-re56lf8l/wheels/d1/17/05/850ab10c33284a4763b0595cd8ea9d01fce6e221cac24b3c01\n",
            "Successfully built unsloth\n",
            "Installing collected packages: unsloth\n",
            "  Attempting uninstall: unsloth\n",
            "    Found existing installation: unsloth 2025.3.14\n",
            "    Uninstalling unsloth-2025.3.14:\n",
            "      Successfully uninstalled unsloth-2025.3.14\n",
            "Successfully installed unsloth-2025.3.14\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "unsloth"
                ]
              },
              "id": "945510ac23dc4240b0e26c703260c2cd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 从unsloth库中导入FastLanguageModel类\n",
        "# unsloth是一个优化的语言模型加载和训练库\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# 设置模型参数\n",
        "# 最大序列长度，即模型能处理的最大token数量\n",
        "max_seq_length = 2048\n",
        "dtype = None # 数据类型设置为None，让模型自动选择合适的数据类型\n",
        "load_in_4bit = True # 启用4bit量化加载 # 4bit量化可以显著减少模型内存占用，但可能略微影响模型性能\n",
        "\n",
        "# 加载预训练模型和分词器\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/gemma-3-4b-it\",\n",
        "    max_seq_length = max_seq_length,     # 设置最大序列长度\n",
        "    dtype = dtype,# 设置数据类型\n",
        "    load_in_4bit = load_in_4bit,  # 启用4bit量化加载\n",
        "    token = hf_token, # 使用Hugging Face的访问令牌\n",
        ")\n",
        "\n",
        "prompt = \"\"\"Below is SQL query. Think like sql expert and generate a summary of the query which explains the use case of the query. As in\n",
        "what the query is trying to read from the database in a usecase sense.\n",
        "\n",
        "### Query:\n",
        "SELECT\n",
        "    c.customer_id,\n",
        "    c.name AS customer_name,\n",
        "    COUNT(o.order_id) AS total_orders,\n",
        "    SUM(o.total_amount) AS total_spent,\n",
        "    AVG(o.total_amount) AS avg_order_value,\n",
        "    MAX(o.order_date) AS last_order_date\n",
        "FROM customers c\n",
        "JOIN orders o ON c.customer_id = o.customer_id\n",
        "LEFT JOIN order_items oi ON o.order_id = oi.order_id\n",
        "WHERE o.order_date >= '2024-01-01'\n",
        "GROUP BY c.customer_id, c.name\n",
        "ORDER BY total_spent DESC\n",
        "LIMIT 10;\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "# 将模型切换到推理模式\n",
        "FastLanguageModel.for_inference(model)\n",
        "# 对输入文本进行分词和编码\n",
        "# [prompt] - 将prompt放入列表中，因为tokenizer期望批处理输入\n",
        "# return_tensors=\"pt\" - 返回PyTorch张量格式\n",
        "# to(\"cuda\") - 将张量移动到GPU上进行计算\n",
        "inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# 使用模型生成响应\n",
        "outputs = model.generate(\n",
        "    input_ids=inputs.input_ids,\n",
        "    attention_mask=inputs.attention_mask,\n",
        "    max_new_tokens=1200,\n",
        "    use_cache=True,\n",
        ")\n",
        "# 后处理生成的输出\n",
        "# batch_decode - 将标记ID序列解码回文本\n",
        "# split(\"### Response:\")[1] - 提取\"### Response:\"之后的部分，即模型的实际回答\n",
        "response = tokenizer.batch_decode(outputs)\n",
        "print(response[0].split(\"### Response:\")[1])\n",
        "\n",
        "#微调前：模型能给出基本的查询解释，但不够精确\n",
        "\n",
        "train_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n",
        "Write a response that appropriately completes the request.\n",
        "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
        "\n",
        "### Instruction:\n",
        "You are a SQL expert with advance understanding of SQL queries. You can understand database schema from the query. Think like sql expert and generate a summary of the query which explains the use case of the query. As in\n",
        "what the query is trying to read from the database in a usecase sense.\n",
        "\n",
        "### Query:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "<think>\n",
        "{}\n",
        "</think>\n",
        "{}\"\"\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "-OD_R_rRlJc5",
        "outputId": "948d50bf-6b23-4a67-891b-759b84e0933a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'hf_token' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1ac85a77a8d6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m# 设置数据类型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mload_in_4bit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_in_4bit\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# 启用4bit量化加载\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhf_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# 使用Hugging Face的访问令牌\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'hf_token' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!wget \"https://huggingface.co/datasets/b-mc2/sql-create-context/resolve/main/sql_create_context_v4.json\"\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"json\", data_files=\"/content/sql_create_context_v4.json\", split=\"train[0:1000]\")\n",
        "# - split: 指定要加载的数据集切片\n",
        "#   - \"train[0:500]\" 表示只加载训练集的前500条数据\n",
        "#   - 这种切片方式可以用于快速实验和调试\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n",
        "\n",
        "#原数据集：输入是英文描述，输出是SQL反转后：输入是SQL(examples[\"answer\"])，输出是英文描述(examples[\"question\"])\n",
        "def switch_and_format_prompt(examples):\n",
        "    inputs = examples[\"question\"] # 使用 answer(SQL) 作为输入\n",
        "    context = examples[\"context\"]\n",
        "    outputs = examples[\"answer\"] # 使用 question(英文描述) 作为输出\n",
        "    texts = []\n",
        "    for input, context, output in zip(inputs, context, outputs):\n",
        "        text = train_prompt_style.format(input, context, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return {\n",
        "        \"text\": texts,\n",
        "    }\n",
        "\n",
        "# 应用转换\n",
        "dataset = dataset.map(switch_and_format_prompt, batched = True)\n",
        "\n"
      ],
      "metadata": {
        "id": "9I3Ge_7knrIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,   # LoRA的秩(rank)值，决定了低秩矩阵的维度，较大的r值(如16)可以提供更强的模型表达能力，但会增加参数量和计算开销，较小的r值(如4或8)则会减少参数量，但可能影响模型性能，通常在4-16之间选择,需要在性能和效率之间权衡\n",
        "    target_modules=[ #指定需要应用LoRA微调的模块列表，q_proj, k_proj, v_proj: 注意力机制中的查询、键、值投影层\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\", #注意力输出投影层\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha=16, #缩放参数，用于控制LoRA更新的强度，通常设置为与r相同的值，较大的alpha会增加LoRA的影响力，较小的alpha则会减弱LoRA的影响\n",
        "    lora_dropout=0, #LoRA层的dropout率，0表示不使用dropout，增加dropout可以帮助防止过拟合，但可能影响训练稳定性，在微调时通常设为0或很小的值\n",
        "    bias=\"none\", #是否微调偏置项，\"none\"表示不微调偏置参数，也可以设置为\"all\"或\"lora_only\"来微调不同范围的偏置\n",
        "    use_gradient_checkpointing=\"unsloth\",  # 梯度检查点策略，\"unsloth\"是一种优化的检查点策略，适用于长上下文可以显著减少显存使用，但会略微增加计算时间对处理长文本特别有用\n",
        "    random_state=3407, #随机数种子，控制初始化的随机性，固定种子可以确保实验可重复性\n",
        "    use_rslora=False, #是否使用RSLoRA(Rank-Stabilized LoRA) False表示使用标准LoRARSLoRA是一种改进的LoRA变体，可以提供更稳定的训练\n",
        "    loftq_config=None, #LoftQ配置None表示不使用LoftQ量化LoftQ是一种用于模型量化的技术，可以减少模型大小\n",
        ")\n",
        "\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    ## 训练参数配置\n",
        "    args=TrainingArguments(\n",
        "        # 批处理相关\n",
        "        per_device_train_batch_size=2, # 每个设备（GPU）的训练批次大小\n",
        "        gradient_accumulation_steps=4,# 梯度累积步数，用于模拟更大的批次大小\n",
        "         # 训练步数和预热\n",
        "        warmup_steps=10,# 学习率预热步数，逐步增加学习率\n",
        "        max_steps=60,# 最大训练步数\n",
        "        learning_rate=5e-5,\n",
        "        fp16=not is_bfloat16_supported(), # 如果不支持 bfloat16，则使用 float16\n",
        "        bf16=is_bfloat16_supported(),# 如果支持则使用 bfloat16，通常在新型 GPU 上性能更好\n",
        "        logging_steps=10,# 每10步记录一次日志\n",
        "        optim=\"adamw_8bit\", # 使用8位精度的 AdamW 优化器\n",
        "        weight_decay=0.1,# 权重衰减率，用于防止过拟合\n",
        "        lr_scheduler_type=\"linear\",# 学习率调度器类型，使用线性衰减\n",
        "        seed=3407,# 随机种子，确保实验可重复性\n",
        "        output_dir=\"outputs\", # 模型和检查点的输出目录\n",
        "    ),\n",
        ")\n",
        "\n",
        "trainer_stats = trainer.train()\n"
      ],
      "metadata": {
        "id": "_3jw6xRenuir"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}